---
title: "EDA"
author: "Samantha Hunter"
date: "10/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

The following space is reserved for the introduction.

The packages used in this report are:

```{r}
library(tidyverse)
library(caret)

# use for subset variable selection
library(leaps)

# use this for parallel computing
library(parallel)

# To use Kable for good looking data print out
library(knitr)
```


Reading in the data
```{r}
# read in raw data
data <- read_csv("OnlineNewsPopularity.csv")

# add in day of week variable called "day"
# check every indicator variable for a 1, once it hits a 1
# the new variable day is defined as whatever day it is
# if no 1 are present saves the day variable as "error"
data <- data %>%
  mutate(
    day = if_else(weekday_is_monday == 1, "monday",
                  if_else(weekday_is_tuesday == 1, "tuesday",
                          if_else(weekday_is_wednesday == 1, "wednesday",
                                  if_else(weekday_is_thursday == 1, "thursday",
                                          if_else(weekday_is_friday == 1, "friday",
                                                  if_else(weekday_is_saturday == 1, "saturday",
                                                          if_else(weekday_is_sunday == 1, "sunday","error"))))))),
    topic = if_else(data_channel_is_lifestyle == 1, "lifestyle", 
                  if_else(data_channel_is_entertainment == 1, "entertainment", 
                          if_else(data_channel_is_bus == 1, "business", 
                                  if_else(data_channel_is_socmed == 1, "socialMedia", 
                                          if_else(data_channel_is_tech == 1, "tech",
                                                  if_else(data_channel_is_world == 1, "world", "error"))))))
    )

# remove old day indicators
data <- data %>%
  select(!starts_with("weekday_is_"), -starts_with("data_channel_is_"))

# change to factor
data$day <- as.factor(data$day)
data$topic <- as.factor(data$topic)

#check structure to make sure is successful 
str(data)

#double check for "errors"
table(data$day)
table(data$topic)

# these actually don't have a topic!
err <- data %>% filter(topic == "error")
```


# EDA - overall exploration

Variables to ignore - url 

I did pull out all the variables from this, but I'm just printing the ones that may present problems when we analyze them
```{r}
# too big?
#plot(data)

# creating a function to return Tukey's five number summary plus the mean
getSummaries <- function(vector){
  five_num<-fivenum(vector)
  mean<-mean(vector)
  return(list(five_num, mean))
}

# pulling out the numeric columns into their own data set so we can get Tukey's five number summary plus the mean for each of them
num_cols <- select_if(data, is.numeric)
num_summary <- lapply(num_cols, getSummaries)


attach(num_summary)

# this is concerning because while I'm sure number of shares decreases relatively quickly as days go by, I am also sure that within two weeks is still prime sharing time for articles. The range of days after publication until when number of shares is captured runs from over to years ago to only 8 days. Sometimes things go viral a couple of months after they are published 
timedelta

# 2 - 23 
n_tokens_title

# 0 to 8474, mean = 546.51
n_tokens_content

# empty articles? - I visited a few of the articles and now believe that these are just an error. See 	
# http://mashable.com/2013/02/05/social-tv-chart-2-5/  and http://mashable.com/2013/02/24/oscars-online/ which
# clearly have words in their article. Just something suspect and to keep in mind
empty <- data %>% filter(n_tokens_content == 0)
sum(empty$global_subjectivity)
sum(empty$avg_positive_polarity)

num_videos

#hist(shares) this causes an error for me, I just commented it out so I can knit the doc

detach(num_summary)


sref <- data %>% filter(self_reference_avg_sharess >= 0)
sref

correlation <- cor(num_cols)
correlation

plot(data$timedelta, data$shares)

```


# EDA - Evan

We can combine our EDAs into one if you'd like or we can do separate either is fine with me. 

Also I think we are supposed to filter for the specific topic before EDA for automation, so I am going to do that here we can adjust this if I am wrong however.
```{r evan eda 1}
# filter data by topic and remove that column and url
filtered_data <- data %>%
  filter(topic == "world") %>%  # the filtering here would need to be automated with params$topic but since this is just for the world part thats what I have here.
  select(-topic,-url)

# Checking correlation with each numeric var and shares

# get names of all columns except day because that isn't numeric, and shares as that is the target variable
col_names <- names(filtered_data)[1:46]

# Build a nice looking tibble for checking correlation
for(i in 1:length(col_names)){
  
  #check correlation between shares and other columns
  correlate <- cor(filtered_data$shares,filtered_data[,i])
  
  # save name of other column
  cor_name <- col_names[i]
  
  # create row of a tibble
  correlation_row <- tibble_row(cor_name,correlate)
  
  # if first iteration of loop, correlation_tibble is the correlation_row
  # if not first iteration, correlation_tibble is binded with the new row info
  if(i == 1){
    correlation_tibble <- correlation_row
  }else{
    correlation_tibble <- rbind(correlation_tibble,correlation_row)
  } # end if else statement
  
  } # end for loop
  

# Add new column for absolute value of correlation and sort by this column in descending order
# To see which variables have the highest impact on 
correlation_tibble <- correlation_tibble %>%
  mutate(abs_cor = abs(correlate)) %>%
  arrange(-abs_cor)

# change names for better printing
names(correlation_tibble) <- c("Variable","Correlation","Absoulte Correlation")

# use kable for good looking print out
kable(correlation_tibble)

```

From checking the correlation of all variables it appears that the following five variables have the highest correlation with shares:  

1. `r as.character(correlation_tibble[1,1])`  
1. `r as.character(correlation_tibble[2,1])`  
1. `r as.character(correlation_tibble[3,1])`  
1. `r as.character(correlation_tibble[4,1])`  
1. `r as.character(correlation_tibble[5,1])`

Lets build some scatter plots to visualize this better!

First I want to build a function to make plotting easier.

```{r evan plot_func}

# Function to build scatter plot with a lm smooth line. 
# cor_num corresponds to the place the place in the correlation tibble created earlier,
# so if cor_num = 1, that is the highest absolute value correlation etc.
plot_func <- function(cor_num){
  
# Which column in the filtered_data tibble is this correlation
col_num <- which(names(filtered_data) %in% as.character(correlation_tibble[cor_num,1]))

# Create new tibble with shares and only the column needed for this plot
data_for_plot <- filtered_data %>%
  select(shares,col_num)

# Rename the second column for easy plotting
names(data_for_plot)[2] <- "need"

# Create plot, this will create a scatter plot between the two variables, with a lm geom smooth line included.
# this will also define the labels and titles correctly.
scat <- data_for_plot %>%
  ggplot(aes(x = shares, y = need))+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(x = "Number of Shares",
       y = str_to_upper(as.character(correlation_tibble[cor_num,1])),
       title = paste0("Visualizing Correlation Between Shares and ",str_to_upper(as.character(correlation_tibble[cor_num,1]))))

# Return the scatter plot
return(scat)
} # end function
```


Now that I have built the function lets check out these plots!

```{r evan cor plot 1}
# Build a plot with the highest absolute value correlation
plot_func(1)
```

`r paste0("Looking at this plot between the number of shares of an article and ", as.character(correlation_tibble[1,1])," we can see that as ",as.character(correlation_tibble[1,1]), " goes up, the number of times an article is shared generally ", if_else(correlation_tibble[1,2] > 0,"increases","descreases"),".")`



```{r evan cor plot 2}
# Build a plot with the 2nd highest absolute value correlation
plot_func(2)
```

`r paste0("Looking at this plot between the number of shares of an article and ", as.character(correlation_tibble[2,1])," we can see that as ",as.character(correlation_tibble[2,1]), " goes up, the number of times an article is shared generally ", if_else(correlation_tibble[2,2] > 0,"increases","descreases"),".")`

```{r evan cor plot 3}
# Build a plot with the 3rd highest absolute value correlation
plot_func(3)
```

`r paste0("Looking at this plot between the number of shares of an article and ", as.character(correlation_tibble[3,1])," we can see that as ",as.character(correlation_tibble[3,1]), " goes up, the number of times an article is shared generally ", if_else(correlation_tibble[3,2] > 0,"increases","descreases"),".")`

```{r evan cor plot 4}
# Build a plot with the 4th highest absolute value correlation
plot_func(4)
```

`r paste0("Looking at this plot between the number of shares of an article and ", as.character(correlation_tibble[4,1])," we can see that as ",as.character(correlation_tibble[4,1]), " goes up, the number of times an article is shared generally ", if_else(correlation_tibble[4,2] > 0,"increases","descreases"),".")`


```{r evan cor plot 5}
# Build a plot with the 5th highest absolute value correlation
plot_func(5)
```

`r paste0("Looking at this plot between the number of shares of an article and ", as.character(correlation_tibble[5,1])," we can see that as ",as.character(correlation_tibble[5,1]), " goes up, the number of times an article is shared generally ", if_else(correlation_tibble[5,2] > 0,"increases","descreases"),".")`


Now that I have looked at the numeric variables, I want to check out the day variable to see if the day of the week has any effect on the number of times an article was shared. To do this I am going to preform a Chi-Squared test! Before I can preform that test however, I am going to create a categorical version of the shares variable called shares_quart that assigns the number of shares to its quartile value. I.E if an articles shares fall into the first quartile it will be assigned "1" etc.

Then I build a contingency table between what day of the week an article was published and what quartile it ends up in. Fianlly I preform the Chi-Sqaured test.
```{r evan quartiles}

# Keep just the day and number of shares
data_for_chi <- filtered_data %>%
  select(day, shares)

# split data into quartiles
quarts <- summary(data_for_chi$shares)

# add column for which quartile the shares fall into.
data_for_chi <- data_for_chi %>%
  mutate(shares_quart = if_else(shares <= quarts[2],"Q1",
                                if_else(shares <= quarts[3],"Q2",
                                        if_else(shares <= quarts[5],"Q3",
                                                if_else(shares <= quarts[6],"Q4","error")))))

# Show contingency table between the day and the shares quartile
kable(table(data_for_chi$shares_quart,data_for_chi$day))

# preform chi squared test
chi_tst <- chisq.test(x = data_for_chi$day, y = data_for_chi$shares_quart)

# Show results
print(chi_tst)
```

`r paste0("The Chi-Squared test resulted in a P-Value of ", chi_tst$p.value,". Using a signifigance level of 0.05 it appears that the day of the week that an article is published ", if_else(chi_tst$p.value > 0.05, "does not", "does"), " have a statistically significant impact on the amount of times an article is shared.") `


# Preprocessing the Data for Manipulation

```{r}
# creating the new data set that only references the desired topic, and then we obviously don't need the topic as it's own variable anymore (they'll all contain the same value)
world <- data %>% filter(topic == "world") %>% select(!topic)

# split the data into a training and test set 
set.seed(214)

# splitting it into 80% for training and 20% for testing
train <- sample(1:nrow(world), size = nrow(world)*0.8)
test <- setdiff(1:nrow(world), train)

# Actually subsetting
train_set <- world[train, 1:49]
test_set <- world[test, ]

# center and scale the data (don't have to use if we don't want to)
preProcValues <- preProcess(train_set, method = c("center", "scale"))
train_set_std <- predict(preProcValues, train_set)


```


# Model Analysis

### Linear Modelling 


Linear regression is a simple supervised learning method for quantitative response variables. These models are well-liked because they are easy to interpret. The 'fit' of a linear model is done by minimizing the sum of the squared residuals. We hope to use this linear model to predict the number of shares an article can achieve based on some predictors. 

We have 50 predictors. Three of these predictors are character variables - url of the article, the day the article was published (day), and the topic of the article (topic). 


```{r}
# Adjusted R Square is 0.03 & Residual Std Error = 0.59
# Horrible
lm_full <- lm(shares~.-url, data=train_set)
summary(lm_full)
# also horrible
plot(lm_full)


lm_sig <- lm(shares ~ n_tokens_title + n_non_stop_words + num_hrefs + num_imgs
             + average_token_length + kw_min_avg + kw_max_avg + kw_avg_avg + max_negative_polarity, 
             data = train_set)
summary(lm_sig)

lm_day <- lm(shares ~ day, data=train_set_std)
summary(lm_day)

lm_lda <- lm(shares~LDA_00+LDA_01+LDA_02+LDA_03+LDA_04, data=train_set_std)
summary(lm_lda)

```


### Random Forests

Setting up parallel processing to reduce the time it takes to run random forests. 

```{r cache=FALSE}
### I have 8 cores, but this is from lecture - sam
#library(parallel)
#detectCores()
#cluster <- makeCluster(4)


# Mark's link from Moodle
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

## All subsequent models are then run in parallel
model <- train(shares ~ LDA_00+LDA_01+LDA_02+LDA_03+LDA_04, data = train_set, method = "rf", 
             trControl = trainControl
             (method = "repeatedcv", number = 5, repeats = 3), 
               preProcess = c("center", "scale"), 
               tuneGrid = data.frame(mtry = 1:15))

## When you are done:
stopCluster(cl)

# Hopefully we can bring this number wayyyyyy down. 
rf_pred <- predict(model, newdata = train_set)
treeRMSE <- sqrt(mean((rf_pred-train_set$shares)^2))
treeRMSE
```


