---
title: "EDA"
author: "Samantha Hunter"
date: "10/17/2021"
output: html_document
---
```{r}
library(tidyverse)
library(caret)

# use for subset variable selection
library(leaps)

# use this for parallel computing
library(parallel)
```


Reading in the data
```{r}
# read in raw data
data <- read_csv("OnlineNewsPopularity.csv")

# add in day of week variable called "day"
# check every indicator variable for a 1, once it hits a 1
# the new variable day is defined as whatever day it is
# if no 1 are present saves the day variable as "error"
data <- data %>%
  mutate(
    day = if_else(weekday_is_monday == 1, "monday",
                  if_else(weekday_is_tuesday == 1, "tuesday",
                          if_else(weekday_is_wednesday == 1, "wednesday",
                                  if_else(weekday_is_thursday == 1, "thursday",
                                          if_else(weekday_is_friday == 1, "friday",
                                                  if_else(weekday_is_saturday == 1, "saturday",
                                                          if_else(weekday_is_sunday == 1, "sunday","error"))))))),
    topic = if_else(data_channel_is_lifestyle == 1, "lifestyle", 
                  if_else(data_channel_is_entertainment == 1, "entertainment", 
                          if_else(data_channel_is_bus == 1, "business", 
                                  if_else(data_channel_is_socmed == 1, "socialMedia", 
                                          if_else(data_channel_is_tech == 1, "tech",
                                                  if_else(data_channel_is_world == 1, "world", "error"))))))
    )

# remove old day indicators
data <- data %>%
  select(!starts_with("weekday_is_"), -starts_with("data_channel_is_"))

# change to factor
data$day <- as.factor(data$day)
data$topic <- as.factor(data$topic)

#check structure to make sure is successful 
str(data)

#double check for "errors"
table(data$day)
table(data$topic)

# these actually don't have a topic!
err <- data %>% filter(topic == "error")
```


# EDA - overall exploration

Variables to ignore - url 

I did pull out all the variables from this, but I'm just printing the ones that may present problems when we analyze them
```{r}
# too big?
#plot(data)

# creating a function to return Tukey's five number summary plus the mean
getSummaries <- function(vector){
  five_num<-fivenum(vector)
  mean<-mean(vector)
  return(list(five_num, mean))
}

# pulling out the numeric columns into their own data set so we can get Tukey's five number summary plus the mean for each of them
num_cols <- select_if(data, is.numeric)
num_summary <- lapply(num_cols, getSummaries)


attach(num_summary)

# this is concerning because while I'm sure number of shares decreases relatively quickly as days go by, I am also sure that within two weeks is still prime sharing time for articles. The range of days after publication until when number of shares is captured runs from over to years ago to only 8 days. Sometimes things go viral a couple of months after they are published 
timedelta

# 2 - 23 
n_tokens_title

# 0 to 8474, mean = 546.51
n_tokens_content

# empty articles? - I visited a few of the articles and now believe that these are just an error. See 	
# http://mashable.com/2013/02/05/social-tv-chart-2-5/  and http://mashable.com/2013/02/24/oscars-online/ which
# clearly have words in their article. Just something suspect and to keep in mind
empty <- data %>% filter(n_tokens_content == 0)
sum(empty$global_subjectivity)
sum(empty$avg_positive_polarity)

num_videos

detach(num_summary)


sref <- data %>% filter(self_reference_avg_sharess >= 0)
sref

correlation <- cor(num_cols)
correlation

plot(data$timedelta, data$shares)

```


# Preprocessing the Data for Manipulation

```{r}
# creating the new data set that only references the desired topic, and then we obviously don't need the topic as it's own variable anymore (they'll all contain the same value)
world <- data %>% filter(topic == "world") %>% select(!topic)

# split the data into a training and test set 
set.seed(214)

# splitting it into 80% for training and 20% for testing
train <- sample(1:nrow(world), size = nrow(world)*0.8)
test <- setdiff(1:nrow(world), train)

# Actually subsetting
train_set <- world[train, 1:49]
test_set <- world[test, ]

# center and scale the data (don't have to use if we don't want to)
preProcValues <- preProcess(train_set, method = c("center", "scale"))
train_set_std <- predict(preProcValues, train_set)


```


# Model Analysis

### Linear Modelling 


Linear regression is a simple supervised learning method for quantitative response variables. These models are well-liked because they are easy to interpret. The 'fit' of a linear model is done by minimizing the sum of the squared residuals. We hope to use this linear model to predict the number of shares an article can achieve based on some predictors. 

We have 50 predictors. Three of these predictors are character variables - url of the article, the day the article was published (day), and the topic of the article (topic). 


```{r}
# Adjusted R Square is 0.03 & Residual Std Error = 0.59
# Horrible
lm_full <- lm(shares~.-url, data=train_set)
summary(lm_full)
# also horrible
plot(lm_full)


lm_sig <- lm(shares ~ n_tokens_title + n_non_stop_words + num_hrefs + num_imgs + average_token_length + kw_min_avg + kw_max_avg + kw_avg_avg + max_negative_polarity, 
             data = train_set)
summary(lm_sig)

lm_day <- lm(shares ~ day, data=train_set_std)
summary(lm_day)

lm_lda <- lm(shares~LDA_00+LDA_01+LDA_02+LDA_03+LDA_04, data=train_set_std)
summary(lm_lda)

```


### Random Forests

Setting up parallel processing to reduce the time it takes to run random forests. 

```{r}
### I have 8 cores, but this is from lecture - sam
#library(parallel)
#detectCores()
#cluster <- makeCluster(4)


# Mark's link from Moodle
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

## All subsequent models are then run in parallel
model <- train(shares ~ day, data = train_set, method = "rf")

rfFit <- train(myHeartDisease ~ ., data = train, method = "rf", 
              trControl = trainControl
               (method = "repeatedcv", number = 5, repeats = 3), 
              preProcess = c("center", "scale"), 
              tuneGrid = data.frame(mtry = 1:15))
rfFit

## When you are done:
stopCluster(cl)

```


